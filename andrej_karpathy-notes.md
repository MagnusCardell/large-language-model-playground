https://www.youtube.com/watch?v=kCc8FmEb1nY

Large Language Model
- probabilistic output following Enlighs language

"Attention is All You Need" paper from 2017
- Generatively Pretrained Transformer
- Transformer described had huge impact 

Goal
- Train a transformer based language model
- Character level LM

Training data set - TinyShakespear 

NanoGpt